% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/lasso.R
\name{lasso}
\alias{lasso}
\title{Lasso Regression (c++)}
\usage{
lasso(X, Y, lam = 0.1, crit = "obj", tol = 1e-04, maxit = 10000,
  ind = NULL)
}
\arguments{
\item{X}{matrix or data frame}

\item{lam}{tuning parameter for lasso regularization term. Defaults to 'lam = 0.1'}

\item{crit}{criterion for convergence. Criterion \code{obj} will loop until the change in the objective after an iteration over the parameter set is less than \code{tol}. Criterion \code{max} will loop until the maximum change in the estimate after an iteration over the parameter set is less than \code{tol}. Defaults to \code{obj}.}

\item{tol}{tolerance for algorithm convergence. Defaults to 1e-4}

\item{maxit}{maximum iterations. Defaults to 1e4}

\item{ind}{optional matrix specifying which coefficients will be penalized.}

\item{y}{matrix or data frame of response values}
}
\value{
returns list of returns which includes:
\item{Iterations}{number of iterations.}
\item{Objective}{value of the objective function.}
\item{Coefficients}{estimated regression coefficients.}
}
\description{
Computes the coefficient estimates for lasso-penalized linear regression.
}
\details{
For details on the implementation of 'GLASSO', see the vignette
\url{https:#mgallow.github.io/GLASSO/}.
}
\references{
\itemize{
\item 
For more information on the ADMM algorithm, see: \cr
Boyd, Stephen, Neal Parikh, Eric Chu, Borja Peleato, Jonathan Eckstein, and others. 2011. 'Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers.' \emph{Foundations and Trends in Machine Learning} 3 (1). Now Publishers, Inc.: 1-122.\cr
\url{https:#web.stanford.edu/~boyd/papers/pdf/admm_distr_stats.pdf}
}
}
\author{
Matt Galloway \email{gall0441@umn.edu}
}
