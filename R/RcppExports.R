# Generated by using Rcpp::compileAttributes() -> do not edit by hand
# Generator token: 10BE3573-1514-4C36-9D1C-5A225CD40393

#' @title Lasso Regression (c++)
#' 
#' @description Computes the coefficient estimates for lasso-penalized linear regression.
#' 
#' @details For details on the implementation of 'GLASSO', see the vignette
#' \url{https://mgallow.github.io/GLASSO/}.
#'
#' @param X matrix or data frame
#' @param Y matrix or data frame of response values
#' @param ind optional matrix specifying which coefficients will be penalized.
#' @param lam tuning parameter for lasso regularization term. Defaults to 'lam = 0.1'
#' @param crit criterion for convergence. Criterion \code{loss} will loop until the change in the objective after an iteration over the parameter set is less than \code{tol}. Criterion \code{max} will loop until the maximum change in the estimate after an iteration over the parameter set is less than \code{tol}. Defaults to \code{max}.
#' @param tol tolerance for algorithm convergence. Defaults to 1e-4
#' @param maxit maximum iterations. Defaults to 1e4
#' 
#' @return returns list of returns which includes:
#' \item{Iterations}{number of iterations.}
#' \item{Loss}{value of the objective function.}
#' \item{Coefficients}{estimated regression coefficients.}
#' \item{H}{update H matrix.}
#' 
#' @references
#' \itemize{
#' \item 
#' For more information on the ADMM algorithm, see: \cr
#' Boyd, Stephen, Neal Parikh, Eric Chu, Borja Peleato, Jonathan Eckstein, and others. 2011. 'Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers.' \emph{Foundations and Trends in Machine Learning} 3 (1). Now Publishers, Inc.: 1-122.\cr
#' \url{https://web.stanford.edu/~boyd/papers/pdf/admm_distr_stats.pdf}
#' }
#' 
#' @author Matt Galloway \email{gall0441@@umn.edu}
#' 
#' @keywords internal
#'
lassoc <- function(X, Y, ind, lam = 0.1, crit = "max", tol = 1e-4, maxit = 1e4) {
    .Call('_GLASSO_lassoc', PACKAGE = 'GLASSO', X, Y, ind, lam, crit, tol, maxit)
}

